import os
import re
import json
from typing import Optional, List, Dict, Any
import tempfile
from pathlib import Path
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

from fastapi import FastAPI, UploadFile, File, HTTPException, Body, Depends
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from sqlalchemy import Column, Integer, String, DateTime, Text, ForeignKey, create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from datetime import datetime

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.embeddings import Embeddings
from langchain_huggingface import HuggingFaceEmbeddings  # BGE ì‚¬ìš©
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.messages import HumanMessage
# from huggingface_hub import login  # BGE ì‚¬ìš©ì‹œ ì£¼ì„ í•´ì œ
from pypdf import PdfReader


os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
# torch ë³´ì•ˆ ê²½ê³  ìš°íšŒ
os.environ.setdefault("PYTORCH_DISABLE_WARN_LOAD", "1")

# API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì½ê¸°)
openai_key = os.getenv("OPENAI_API_KEY")
hf_token = os.getenv("HUGGINGFACE_TOKEN")

if openai_key:
    os.environ["OPENAI_API_KEY"] = openai_key
# if hf_token:
#     login(hf_token)  # BGE ì‚¬ìš©ì‹œ ì£¼ì„ í•´ì œ

# SQLite ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
DATABASE_URL = "sqlite:///./insurance_system.db"
engine = create_engine(DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë¸ë“¤
class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, unique=True, index=True)
    age = Column(Integer)
    gender = Column(String)
    medical_history = Column(Text)
    created_at = Column(DateTime, default=datetime.utcnow)

class Insurance(Base):
    __tablename__ = "insurances"
    
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)  # PDF íŒŒì¼ëª…
    type = Column(String, index=True)  # ìƒëª…/ì†í•´/ìë™ì°¨
    file_path = Column(String)         # ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    user_id = Column(Integer, ForeignKey("users.id"), index=True)  # ì‚¬ìš©ìë³„ ë³´í—˜ ë¶„ë¦¬
    embedding_model = Column(String, default="openai", index=True)  # ì‚¬ìš©ëœ ì„ë² ë”© ëª¨ë¸
    created_at = Column(DateTime, default=datetime.utcnow)

# ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„±
Base.metadata.create_all(bind=engine)

# ë°ì´í„°ë² ì´ìŠ¤ ì˜ì¡´ì„±
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

app = FastAPI(
    title="ë³´í—˜ ì²­êµ¬ ì‹¬ì‚¬ ìë™í™” ì‹œìŠ¤í…œ API",
    description="ë³´í—˜ ì•½ê´€ PDF ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ë° ì‹¬ì‚¬ ì§€ì›",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# BGE-M3 ì„ë² ë”© í´ë˜ìŠ¤ (safetensors ì‚¬ìš©, torch ë³´ì•ˆ ìš°íšŒ)
class LazyBgeEmbeddings(Embeddings):
    """BAAI/bge-m3 ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©ì„ ìœ„í•œ ì§€ì—° ë¡œë”© í´ë˜ìŠ¤ (safetensors)"""
    def __init__(self, model_name: str = "BAAI/bge-m3", device: str = "cpu", normalize: bool = True):
        self.model_name = model_name
        self.device = device
        self.normalize = normalize
        self._model = None
        self._tokenizer = None
        self.query_instruction = "Represent this sentence for searching relevant passages: "
        self.passage_instruction = "Represent this passage for retrieval: "

    def _ensure_model(self):
        if self._model is None or self._tokenizer is None:
            try:
                import torch
                from transformers import AutoTokenizer, AutoModel
                
                # safetensors ì‚¬ìš©ìœ¼ë¡œ torch ë³´ì•ˆ ë¬¸ì œ ìš°íšŒ
                self._model = AutoModel.from_pretrained(
                    self.model_name,
                    use_safetensors=True,
                    trust_remote_code=True
                )
                self._tokenizer = AutoTokenizer.from_pretrained(self.model_name)
                self._model.eval()
                
                print(f"âœ… BGE-M3 ëª¨ë¸ì´ safetensorsë¡œ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                print(f"âš ï¸ BGE-M3 ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"BGE-M3 ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        self._ensure_model()
        inst_texts = [f"{self.passage_instruction}{t}" for t in texts]
        
        import torch
        embeddings = []
        
        for text in inst_texts:
            inputs = self._tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
            
            with torch.no_grad():
                outputs = self._model(**inputs)
                # í‰ê·  í’€ë§ìœ¼ë¡œ ì„ë² ë”© ìƒì„±
                embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
                
                if self.normalize:
                    embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)
                
                embeddings.append(embedding.tolist())
        
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        self._ensure_model()
        q = f"{self.query_instruction}{text}"
        
        import torch
        inputs = self._tokenizer(q, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = self._model(**inputs)
            embedding = outputs.last_hidden_state.mean(dim=1).squeeze()
            
            if self.normalize:
                embedding = torch.nn.functional.normalize(embedding, p=2, dim=0)
            
            return embedding.tolist()


# Qwen ë¡œì»¬ LLM í´ë˜ìŠ¤
class LazyQwenLLM:
    """Qwen ë¡œì»¬ LLM ì‚¬ìš©ì„ ìœ„í•œ ì§€ì—° ë¡œë”© í´ë˜ìŠ¤"""
    def __init__(self, model_name: str = "Qwen/Qwen1.5-0.5B-Chat"):
        self.model_name = model_name
        self._model = None
        self._tokenizer = None

    def _ensure_model(self):
        if self._model is None or self._tokenizer is None:
            try:
                import torch
                from transformers import AutoTokenizer, AutoModelForCausalLM
                
                self._tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)
                self._model = AutoModelForCausalLM.from_pretrained(
                    self.model_name, 
                    trust_remote_code=True, 
                    torch_dtype=torch.float32
                ).to("cpu")
                
                # ìƒì„± ì„¤ì • - ì°¸ê³  íŒŒì¼ê³¼ ë™ì¼í•˜ê²Œ
                gc = self._model.generation_config
                gc.do_sample = False
                gc.temperature = None
                gc.top_p = None
                gc.top_k = None
                gc.typical_p = None
                gc.num_beams = 1
                gc.use_cache = True
                self._model.generation_config = gc
                
                print(f"âœ… Qwen ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                print(f"âš ï¸ Qwen ë¡œë“œ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"Qwen ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    def generate_answer(self, prompt: str, max_input_tokens: int = 1536, max_new_tokens: int = 128) -> str:
        self._ensure_model()
        
        import torch
        enc = self._tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_input_tokens)
        input_ids = enc.input_ids.to(self._model.device)
        attention_mask = enc.attention_mask.to(self._model.device)
        
        input_length = input_ids.shape[1]
        
        with torch.no_grad():
            out = self._model.generate(
                input_ids=input_ids,
                attention_mask=attention_mask,
                do_sample=False,  # greedy decoding
                num_beams=1,
                max_new_tokens=max_new_tokens,
                pad_token_id=self._tokenizer.eos_token_id,
                eos_token_id=self._tokenizer.eos_token_id,
                use_cache=True
            )
        
        # ì „ì²´ ì‘ë‹µì„ ë””ì½”ë”©
        full_response = self._tokenizer.decode(out[0], skip_special_tokens=True)
        
        # ì…ë ¥ í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ë§Œ ì¶”ì¶œ
        original_prompt = self._tokenizer.decode(input_ids[0], skip_special_tokens=True)
        
        if full_response.startswith(original_prompt):
            response = full_response[len(original_prompt):].strip()
        else:
            response = full_response.strip()
        
        # ì‘ë‹µì´ ì—†ê±°ë‚˜ ì˜ë¯¸ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
        if not response or len(response) < 2:
            response = "ë³´ë¥˜"
        
        return response


_BGE_LAZY: Optional[LazyBgeEmbeddings] = None
_QWEN_LAZY: Optional[LazyQwenLLM] = None

def get_bge_embeddings() -> LazyBgeEmbeddings:
    global _BGE_LAZY
    if _BGE_LAZY is None:
        _BGE_LAZY = LazyBgeEmbeddings(model_name="BAAI/bge-m3", device="cpu", normalize=True)
    return _BGE_LAZY

def get_qwen_llm() -> LazyQwenLLM:
    global _QWEN_LAZY
    if _QWEN_LAZY is None:
        _QWEN_LAZY = LazyQwenLLM(model_name="Qwen/Qwen1.5-0.5B-Chat")
    return _QWEN_LAZY

def get_embeddings(model_type: str = "openai"):
    """ì„ë² ë”© ëª¨ë¸ ì„ íƒ"""
    if model_type == "openai":
        if openai_key:
            return OpenAIEmbeddings()
        else:
            raise HTTPException(
                status_code=400, 
                detail="OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤. .env íŒŒì¼ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            )
    elif model_type == "bge-m3":
        return get_bge_embeddings()
    else:
        raise HTTPException(
            status_code=400,
            detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ëª¨ë¸: {model_type}. ì§€ì› ëª¨ë¸: openai, bge-m3"
        )


# ë³´í—˜ ì¢…ë¥˜ë³„ ë²¡í„° ìŠ¤í† ì–´ ê´€ë¦¬ (ì‚¬ìš©ìë³„)
INSURANCE_TYPES = ["ìƒëª…ë³´í—˜", "ì†í•´ë³´í—˜", "ìë™ì°¨ë³´í—˜"]
# VECTORSTORES[user_id][insurance_type] êµ¬ì¡°
VECTORSTORES = {}


def _normalize_pdf_text(txt: str) -> str:
    if not txt:
        return ""
    # ë¦¬ê°€ì²˜ ì²˜ë¦¬
    lig_map = {
        "\ufb00": "ff", "\ufb01": "fi", "\ufb02": "fl",
        "\ufb03": "ffi", "\ufb04": "ffl"
    }
    for k, v in lig_map.items():
        txt = txt.replace(k, v)
    
    # í•˜ì´í”ˆìœ¼ë¡œ ë¶„ë¦¬ëœ ë‹¨ì–´ ê²°í•©
    txt = re.sub(r"(\w)-\s*\n\s*(\w)", r"\1\2", txt)
    # ê°œí–‰ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜
    txt = re.sub(r"\s*\n\s*", " ", txt)
    # ë‹¤ì¤‘ ê³µë°± ì¶•ì†Œ
    txt = re.sub(r"\s{2,}", " ", txt)
    return txt.strip()


def load_pdf(file_path: str) -> str:
    reader = PdfReader(file_path)
    raw = "\n".join([p.extract_text() for p in reader.pages if p.extract_text()])
    return _normalize_pdf_text(raw)


def get_user_vectorstore(user_id: int, insurance_type: str, embedding_model: str = "openai"):
    """ì‚¬ìš©ìë³„ ë³´í—˜ ì¢…ë¥˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ê°€ì ¸ì˜¤ê¸°"""
    cache_key = f"{user_id}_{insurance_type}_{embedding_model}"
    
    if user_id not in VECTORSTORES:
        VECTORSTORES[user_id] = {}
    
    if cache_key not in VECTORSTORES[user_id]:
        # ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ ì‹œë„
        faiss_path = f"faiss_db/user_{user_id}_{insurance_type}_{embedding_model}"
        if Path(f"{faiss_path}/index.faiss").exists() and Path(f"{faiss_path}/index.pkl").exists():
            try:
                VECTORSTORES[user_id][cache_key] = FAISS.load_local(
                    faiss_path,
                    embeddings=get_embeddings(embedding_model),
                    allow_dangerous_deserialization=True
                )
            except Exception as e:
                print(f"âŒ ì‚¬ìš©ì {user_id} {insurance_type} ({embedding_model}) ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                VECTORSTORES[user_id][cache_key] = None
        else:
            VECTORSTORES[user_id][cache_key] = None
    
    return VECTORSTORES[user_id][cache_key]


def save_user_vectorstore(user_id: int, insurance_type: str, vectorstore, embedding_model: str = "openai"):
    """ì‚¬ìš©ìë³„ ë³´í—˜ ì¢…ë¥˜ë³„ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥"""
    cache_key = f"{user_id}_{insurance_type}_{embedding_model}"
    save_path = f"faiss_db/user_{user_id}_{insurance_type}_{embedding_model}"
    os.makedirs(save_path, exist_ok=True)
    vectorstore.save_local(save_path)
    
    # ë©”ëª¨ë¦¬ì—ë„ ì €ì¥
    if user_id not in VECTORSTORES:
        VECTORSTORES[user_id] = {}
    VECTORSTORES[user_id][cache_key] = vectorstore


# ë”ë¯¸ ì‚¬ìš©ì ì´ˆê¸°í™” í•¨ìˆ˜
def init_dummy_users():
    db = SessionLocal()
    try:
        # ë”ë¯¸ ì‚¬ìš©ìê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        existing_users = db.query(User).count()
        if existing_users == 0:
            # ë”ë¯¸ ì‚¬ìš©ì ìƒì„±
            dummy_users = [
                User(name="í•˜ë™ìš°", age=28, gender="ë‚¨ì„±", medical_history="ê³ í˜ˆì••, ë‹¹ë‡¨ë³‘ ê°€ì¡±ë ¥"),
                User(name="ìµœìš°ì§„", age=32, gender="ë‚¨ì„±", medical_history="ì•Œë ˆë¥´ê¸° ì²´ì§ˆ, ì²œì‹"),
                User(name="ì•ˆí˜„ìš±", age=26, gender="ë‚¨ì„±", medical_history="íŠ¹ì´ì‚¬í•­ ì—†ìŒ")
            ]
            
            for user in dummy_users:
                db.add(user)
            db.commit()
            print("âœ… ë”ë¯¸ ì‚¬ìš©ì 3ëª… ìƒì„± ì™„ë£Œ")
        else:
            print("â„¹ï¸ ê¸°ì¡´ ì‚¬ìš©ì ë°ì´í„° í™•ì¸ë¨")
    finally:
        db.close()

@app.on_event("startup")
async def startup_event():
    # ë”ë¯¸ ì‚¬ìš©ì ì´ˆê¸°í™”
    init_dummy_users()
    # ì‚¬ìš©ìë³„ ë²¡í„° ìŠ¤í† ì–´ëŠ” í•„ìš”ì‹œ ë™ì  ë¡œë“œ
    print("âœ… ë³´í—˜ ì²­êµ¬ ì‹¬ì‚¬ ì‹œìŠ¤í…œ ì‹œì‘ - ì‚¬ìš©ìë³„ ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„ ì™„ë£Œ")


class UserInfo(BaseModel):
    name: str
    age: int
    gender: str
    insurance_type: str
    claim_amount: int
    claim_type: str
    medical_history: Optional[str] = ""
    current_condition: Optional[str] = ""
    additional_info: Optional[str] = ""


class RAGRequest(BaseModel):
    prompt: str = Field(..., description="ê²€ìƒ‰ ì¿¼ë¦¬")
    insurance_type: str = Field(..., description="ë³´í—˜ ì¢…ë¥˜: ìƒëª…ë³´í—˜/ì†í•´ë³´í—˜/ìë™ì°¨ë³´í—˜")
    user_id: int = Field(..., description="ì‚¬ìš©ì ID")
    embedding_model: str = Field("openai", description="ì„ë² ë”© ëª¨ë¸: openai/bge-m3")
    rag_model: str = Field("openai", description="RAG ëª¨ë¸: openai/qwen")
    top_k: int = Field(3, ge=1, le=10, description="ê²€ìƒ‰í•  ê²°ê³¼ ìˆ˜")


class SearchResponse(BaseModel):
    prompt: str
    response: str
    summary: Optional[str] = None  # í•œì¤„ ìš”ì•½ ì¶”ê°€
    pdf_titles: Optional[List[str]] = None

class UserResponse(BaseModel):
    id: int
    name: str
    age: int
    gender: str
    medical_history: str

class InsuranceUploadRequest(BaseModel):
    insurance_type: str = Field(..., description="ë³´í—˜ ì¢…ë¥˜: ìƒëª…ë³´í—˜/ì†í•´ë³´í—˜/ìë™ì°¨ë³´í—˜")


# ì‚¬ìš©ì ê´€ë ¨ API
@app.get("/users", response_model=List[UserResponse], summary="ë”ë¯¸ ì‚¬ìš©ì ëª©ë¡ ì¡°íšŒ")
async def get_users(db: Session = Depends(get_db)):
    users = db.query(User).all()
    return users

@app.get("/users/{user_name}", response_model=UserResponse, summary="íŠ¹ì • ì‚¬ìš©ì ì •ë³´ ì¡°íšŒ")
async def get_user(user_name: str, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.name == user_name).first()
    if not user:
        raise HTTPException(status_code=404, detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return user

@app.get("/insurance-types", summary="ì§€ì›í•˜ëŠ” ë³´í—˜ ì¢…ë¥˜ ëª©ë¡")
async def get_insurance_types():
    return {"insurance_types": INSURANCE_TYPES}

@app.get("/insurances/{insurance_type}", summary="íŠ¹ì • ë³´í—˜ ì¢…ë¥˜ì˜ ì—…ë¡œë“œëœ ì•½ê´€ ëª©ë¡")
async def get_insurances_by_type(insurance_type: str, user_id: int = None, db: Session = Depends(get_db)):
    if insurance_type not in INSURANCE_TYPES:
        raise HTTPException(status_code=400, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³´í—˜ ì¢…ë¥˜ì…ë‹ˆë‹¤.")
    
    if user_id:
        # íŠ¹ì • ì‚¬ìš©ìì˜ ë³´í—˜ë§Œ ì¡°íšŒ
        insurances = db.query(Insurance).filter(
            Insurance.type == insurance_type,
            Insurance.user_id == user_id
        ).all()
    else:
        # ì „ì²´ ì¡°íšŒ (ê¸°ì¡´ í˜¸í™˜ì„±)
        insurances = db.query(Insurance).filter(Insurance.type == insurance_type).all()
    
    return {
        "insurance_type": insurance_type,
        "count": len(insurances),
        "insurances": [{"id": ins.id, "name": ins.name, "embedding_model": ins.embedding_model, "created_at": ins.created_at} for ins in insurances]
    }


@app.post("/upload-insurance", summary="ë³´í—˜ ì¢…ë¥˜ë³„ ì•½ê´€ PDF ì—…ë¡œë“œ")
async def upload_insurance(
    insurance_type: str = Body(...),
    user_id: int = Body(...),
    embedding_model: str = Body("openai"),
    file: UploadFile = File(...),
    db: Session = Depends(get_db)
):
    # ë³´í—˜ ì¢…ë¥˜ ìœ íš¨ì„± ê²€ì‚¬
    if insurance_type not in INSURANCE_TYPES:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³´í—˜ ì¢…ë¥˜ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ì¢…ë¥˜: {INSURANCE_TYPES}")
    
    if not file.filename.lower().endswith(".pdf"):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ í—ˆìš©ë©ë‹ˆë‹¤.")
    
    try:
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
            tmp.write(await file.read())
            tmp_path = tmp.name

        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        paper_text = load_pdf(tmp_path)
        if not paper_text or len(paper_text) < 50:
            raise HTTPException(
                status_code=400, 
                detail="PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìŠ¤ìº”ë³¸ì´ë©´ OCR í›„ ì¬ì—…ë¡œë“œí•˜ì„¸ìš”."
            )

        # í…ìŠ¤íŠ¸ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000, 
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        docs = splitter.create_documents([paper_text])
        
        # ë©”íƒ€ë°ì´í„°ì— ë³´í—˜ ì¢…ë¥˜ì™€ íŒŒì¼ëª… ì¶”ê°€
        for doc in docs:
            doc.metadata["source"] = file.filename
            doc.metadata["insurance_type"] = insurance_type

        # ì„ë² ë”© ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
        valid_models = ["openai", "bge-m3"]
        if embedding_model not in valid_models:
            raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ëª¨ë¸: {valid_models}")

        # ì‚¬ìš©ìë³„ ë³´í—˜ ì¢…ë¥˜ë³„ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±/ì—…ë°ì´íŠ¸
        user_vectorstore = get_user_vectorstore(user_id, insurance_type, embedding_model)
        
        if user_vectorstore is None:
            # ìƒˆ ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            new_vectorstore = FAISS.from_documents(
                docs, 
                embedding=get_embeddings(embedding_model), 
                distance_strategy=DistanceStrategy.COSINE
            )
        else:
            # ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ì— ë¬¸ì„œ ì¶”ê°€
            new_docs_vectorstore = FAISS.from_documents(
                docs, 
                embedding=get_embeddings(embedding_model), 
                distance_strategy=DistanceStrategy.COSINE
            )
            user_vectorstore.merge_from(new_docs_vectorstore)
            new_vectorstore = user_vectorstore

        # ì‚¬ìš©ìë³„ ë³´í—˜ ì¢…ë¥˜ë³„ ë””ë ‰í† ë¦¬ì— ì €ì¥
        save_user_vectorstore(user_id, insurance_type, new_vectorstore, embedding_model)

        # ë°ì´í„°ë² ì´ìŠ¤ì— ë³´í—˜ ì •ë³´ ì €ì¥
        save_path = f"faiss_db/user_{user_id}_{insurance_type}_{embedding_model}"
        insurance_record = Insurance(
            name=file.filename,
            type=insurance_type,
            file_path=save_path,
            user_id=user_id,
            embedding_model=embedding_model
        )
        db.add(insurance_record)
        db.commit()

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.unlink(tmp_path)

        return JSONResponse(content={
            "message": f"{insurance_type} - {file.filename}: {len(docs)}ê°œì˜ ì²­í¬ë¡œ ë²¡í„°í™” ë° ì €ì¥ ì™„ë£Œ ({embedding_model} ì„ë² ë”©)",
            "filename": file.filename,
            "insurance_type": insurance_type,
            "embedding_model": embedding_model,
            "chunks": len(docs)
        })
        
    except HTTPException:
        raise
    except Exception as e:
        if 'tmp_path' in locals():
            os.unlink(tmp_path)
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")


# ê¸°ì¡´ ì—…ë¡œë“œ APIëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (deprecated)
@app.post("/upload-paper", summary="PDF ì—…ë¡œë“œ (í˜¸í™˜ì„±ìš©, deprecated)")
async def upload_pdf_deprecated(file: UploadFile = File(...)):
    # ê¸°ë³¸ì ìœ¼ë¡œ ìƒëª…ë³´í—˜ìœ¼ë¡œ ë¶„ë¥˜
    return await upload_insurance("ìƒëª…ë³´í—˜", file, next(get_db()))


@app.post("/search-similar", response_model=SearchResponse, summary="ë³´í—˜ ì¢…ë¥˜ë³„ ìœ ì‚¬ë„ ê¸°ë°˜ ì•½ê´€ ê²€ìƒ‰")
async def search_similar_terms(rag_request: RAGRequest = Body(...), db: Session = Depends(get_db)):
    # ë³´í—˜ ì¢…ë¥˜ ìœ íš¨ì„± ê²€ì‚¬
    if rag_request.insurance_type not in INSURANCE_TYPES:
        raise HTTPException(status_code=400, detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë³´í—˜ ì¢…ë¥˜ì…ë‹ˆë‹¤. ê°€ëŠ¥í•œ ì¢…ë¥˜: {INSURANCE_TYPES}")
    
    # ì‚¬ìš©ìë³„ í•´ë‹¹ ë³´í—˜ ì¢…ë¥˜ì˜ ë²¡í„°ìŠ¤í† ì–´ í™•ì¸ (ì„ë² ë”© ëª¨ë¸ë³„)
    vectorstore = get_user_vectorstore(rag_request.user_id, rag_request.insurance_type, rag_request.embedding_model)
    if vectorstore is None:
        # ì—…ë¡œë“œëœ ì•½ê´€ì´ ìˆëŠ”ì§€ DBì—ì„œ í™•ì¸ (ì‚¬ìš©ìë³„, ì„ë² ë”© ëª¨ë¸ë³„)
        insurance_count = db.query(Insurance).filter(
            Insurance.type == rag_request.insurance_type,
            Insurance.user_id == rag_request.user_id,
            Insurance.embedding_model == rag_request.embedding_model
        ).count()
        if insurance_count == 0:
            raise HTTPException(
                status_code=400,
                detail=f"ì‚¬ìš©ì {rag_request.user_id}ì˜ {rag_request.insurance_type} ({rag_request.embedding_model} ì„ë² ë”©) ì¢…ë¥˜ ë“±ë¡ëœ ì•½ê´€ì´ ì—†ìŠµë‹ˆë‹¤. ë³´í—˜ ì•½ê´€ì„ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            )
        else:
            raise HTTPException(
                status_code=500,
                detail=f"ì‚¬ìš©ì {rag_request.user_id}ì˜ {rag_request.insurance_type} ({rag_request.embedding_model}) ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            )
    
    try:
        # ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
        fetch_k = max(rag_request.top_k * 3, 10)
        
        # ê¸°ë³¸ ê²€ìƒ‰
        docs_with_scores = vectorstore.similarity_search_with_score(
            rag_request.prompt, 
            k=fetch_k
        )
        docs = [doc for doc, score in docs_with_scores]
        
        # MMRë¡œ ë‹¤ì–‘ì„± í™•ë³´
        try:
            docs = vectorstore.max_marginal_relevance_search(
                rag_request.prompt, 
                k=min(len(docs), rag_request.top_k), 
                fetch_k=fetch_k
            )
        except Exception:
            docs = docs[:rag_request.top_k]
        
        if not docs:
            return SearchResponse(
                prompt=rag_request.prompt,
                response="ê´€ë ¨ ì•½ê´€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                summary="ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë³´í†µ - ê´€ë ¨ ì•½ê´€ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
            )
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context_parts = []
        pdf_titles = set()
        
        for i, doc in enumerate(docs, 1):
            content = doc.page_content.strip()
            source = doc.metadata.get('source', 'Unknown')
            pdf_titles.add(source)
            
            context_parts.append(f"[ë¬¸ì„œ {i}: {source}]\n{content}")
        
        context = "\n\n".join(context_parts)
        
        # RAG ëª¨ë¸ì— ë”°ë¥¸ ë‹µë³€ ìƒì„±
        if rag_request.rag_model == "qwen":
            # Qwen ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©
            try:
                qwen_llm = get_qwen_llm()
                
                # Qwenì—ê²Œ ë§¤ìš° ê°„ë‹¨í•œ ì‹¬ì‚¬ ê²°ê³¼ë§Œ ìš”ì²­
                simple_prompt = f"""ë³´í—˜ ì²­êµ¬: {rag_request.prompt[:100]}

ì•½ê´€ ì •ë³´: {context[:300]}

ìœ„ ì •ë³´ë¡œ ì‹¬ì‚¬ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë‹µí•˜ì„¸ìš”:
ìŠ¹ì¸, ë³´ë¥˜, ê±°ì ˆ

ë‹µë³€:"""
                
                # ê°„ë‹¨í•œ ì‹¬ì‚¬ ê²°ê³¼ ìƒì„±
                result_response = qwen_llm.generate_answer(simple_prompt, max_input_tokens=800, max_new_tokens=50)
                
                print(f"ğŸ” Qwen ì‹¬ì‚¬ê²°ê³¼ ì‘ë‹µ: '{result_response}'")
                
                # ì‹¬ì‚¬ ê²°ê³¼ ì¶”ì¶œ
                if "ìŠ¹ì¸" in result_response:
                    decision = "ìŠ¹ì¸"
                    reason = "ê´€ë ¨ ì•½ê´€ì—ì„œ ë³´ì¥ ì¡°ê±´ì„ ì¶©ì¡±í•˜ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."
                elif "ê±°ì ˆ" in result_response:
                    decision = "ê±°ì ˆ"
                    reason = "ì•½ê´€ìƒ ë³´ì¥ ì œì™¸ ì¡°ê±´ì— í•´ë‹¹í•˜ê±°ë‚˜ ì¦ë¹™ì´ ë¶€ì¡±í•©ë‹ˆë‹¤."
                else:
                    decision = "ë³´ë¥˜"
                    reason = "ì¶”ê°€ ì„œë¥˜ ê²€í†  ë° ì „ë¬¸ê°€ ì‹¬ì‚¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                
                # ê°„ë‹¨í•œ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ êµ¬ì„±
                answer = f"""ğŸ” **ì‹¬ì‚¬ ê²°ê³¼**: {decision}

ğŸ“ **ì‹¬ì‚¬ ì˜ê²¬**: {reason}

ğŸ“‹ **ì°¸ê³  ì•½ê´€**:
{context[:400]}...

â€» ì´ëŠ” Qwen ë¡œì»¬ ëª¨ë¸ì˜ 1ì°¨ ì‹¬ì‚¬ ê²°ê³¼ì´ë©°, ì •í™•í•œ ì‹¬ì‚¬ë¥¼ ìœ„í•´ì„œëŠ” ì „ë¬¸ê°€ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤."""
                
                # ì‹¬ì‚¬ ê²°ê³¼ì— ë”°ë¥¸ ìš”ì•½ ìƒì„±
                if decision == "ìŠ¹ì¸":
                    summary = "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë†’ìŒ - ë³´ì¥ ì¡°ê±´ ì¶©ì¡±ìœ¼ë¡œ ìŠ¹ì¸ ê°€ëŠ¥"
                elif decision == "ê±°ì ˆ":
                    summary = "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë‚®ìŒ - ì•½ê´€ìƒ ë³´ì¥ ì œì™¸ ì¡°ê±´ í•´ë‹¹"
                else:
                    summary = "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë³´í†µ - ì¶”ê°€ ì„œë¥˜ ë° ì „ë¬¸ê°€ ê²€í†  í•„ìš”"
                
            except Exception as e:
                answer = f"""
ê´€ë ¨ ì•½ê´€ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:

{context[:1000]}...

â€» Qwen ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤. ({str(e)})
ìƒì„¸í•œ ì‹¬ì‚¬ë¥¼ ìœ„í•´ ì „ë¬¸ê°€ì˜ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.
"""
                summary = "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë³´í†µ - Qwen ë¶„ì„ ì‹¤íŒ¨ë¡œ ì „ë¬¸ê°€ ê²€í†  í•„ìš”"
                
        elif rag_request.rag_model == "openai" and openai_key:
            # OpenAI ëª¨ë¸ ì‚¬ìš©
            llm = ChatOpenAI(model="gpt-4o", temperature=0.2)
            
            full_prompt = f"""
ë‹¤ìŒì€ ë³´í—˜ ì•½ê´€ì—ì„œ ì¶”ì¶œí•œ ê´€ë ¨ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤. 
ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ë³´í—˜ ì²­êµ¬ ìƒí™©ì— ëŒ€í•œ ì‹¬ì‚¬ ì˜ê²¬ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ê´€ë ¨ ì•½ê´€ ë‚´ìš©:
{context}

ì‚¬ìš©ì ì§ˆì˜: {rag_request.prompt}

ë‹µë³€ í˜•ì‹:
1. ê´€ë ¨ ì•½ê´€ ìš”ì•½
2. ì²­êµ¬ ìŠ¹ì¸ ê°€ëŠ¥ì„± 
3. ì£¼ì˜ì‚¬í•­ì´ë‚˜ ì¶”ê°€ í•„ìš” ì„œë¥˜
4. ì°¸ê³ í•œ ë¬¸ì„œëª…

í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
            
            # í•œì¤„ ìš”ì•½ì„ ìœ„í•œ ë³„ë„ í”„ë¡¬í”„íŠ¸
            summary_prompt = f"""
ë‹¤ìŒ ë³´í—˜ ì²­êµ¬ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ í•œ ë¬¸ì¥ìœ¼ë¡œ ìŠ¹ì¸ ê°€ëŠ¥ì„±ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì²­êµ¬ ìƒí™©: {rag_request.prompt}
ê´€ë ¨ ì•½ê´€: {context[:500]}

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ì˜ í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
- "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë†’ìŒ - [ì£¼ìš” ì´ìœ ]"
- "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë³´í†µ - [ì£¼ìš” ì´ìœ ]" 
- "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë‚®ìŒ - [ì£¼ìš” ì´ìœ ]"

í•œ ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
"""
            
            try:
                # ìƒì„¸ ë‹µë³€ ìƒì„±
                response = llm.invoke([HumanMessage(content=full_prompt)])
                answer = response.content.strip()
                
                # í•œì¤„ ìš”ì•½ ìƒì„±
                summary_response = llm.invoke([HumanMessage(content=summary_prompt)])
                summary = summary_response.content.strip()
                
            except Exception as e:
                # OpenAI í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
                answer = f"""
ê´€ë ¨ ì•½ê´€ ë‚´ìš©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:

{context[:1000]}...

â€» AI ëª¨ë¸ í˜¸ì¶œ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤. ({str(e)})
ìƒì„¸í•œ ì‹¬ì‚¬ë¥¼ ìœ„í•´ ì „ë¬¸ê°€ì˜ ê²€í† ê°€ í•„ìš”í•©ë‹ˆë‹¤.
"""
                summary = "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë³´í†µ - AI ë¶„ì„ ì‹¤íŒ¨ë¡œ ì „ë¬¸ê°€ ê²€í†  í•„ìš”"
        else:
            # OpenAI í‚¤ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì‘ë‹µ
            answer = f"""
ê´€ë ¨ ì•½ê´€ ë‚´ìš©:

{context[:1500]}

â€» ìƒì„¸í•œ AI ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenAI API í‚¤ ì„¤ì •ì´ë‚˜ Qwen ëª¨ë¸ ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤.
ìœ„ ì•½ê´€ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì „ë¬¸ê°€ì˜ ê²€í† ë¥¼ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""
            summary = "ì‹¬ì‚¬ í†µê³¼ í™•ë¥  ë³´í†µ - ì „ë¬¸ê°€ ê²€í†  í•„ìš” (AI ë¶„ì„ ë¶ˆê°€)"
        
        return SearchResponse(
            prompt=rag_request.prompt,
            response=answer,
            summary=summary,
            pdf_titles=list(pdf_titles)
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")


@app.post("/rag-query", response_model=SearchResponse, summary="RAG ì§ˆì˜ (í˜¸í™˜ì„±)")
async def rag_query(rag_request: RAGRequest = Body(...)):
    """ê¸°ì¡´ RAG APIì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ì—”ë“œí¬ì¸íŠ¸"""
    return await search_similar_terms(rag_request)


@app.get("/debug-stats", summary="ì‚¬ìš©ìë³„ ë³´í—˜ ì¢…ë¥˜ë³„ ìƒíƒœ í™•ì¸")
async def debug_stats(user_id: int = None, db: Session = Depends(get_db)):
    result = {
        "server_status": "running",
        "embeddings_model": "OpenAI",
        "insurance_types": {}
    }
    
    if user_id:
        # íŠ¹ì • ì‚¬ìš©ìì˜ ë³´í—˜ í†µê³„
        for insurance_type in INSURANCE_TYPES:
            # DBì—ì„œ íŒŒì¼ ìˆ˜ ì¡°íšŒ
            file_count = db.query(Insurance).filter(
                Insurance.type == insurance_type,
                Insurance.user_id == user_id
            ).count()
            
            # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ìƒíƒœ í™•ì¸
            user_vectorstore = get_user_vectorstore(user_id, insurance_type)
            loaded = user_vectorstore is not None
            
            result["insurance_types"][insurance_type] = {
                "loaded": loaded,
                "docs": file_count  # íŒŒì¼ ìˆ˜ë¡œ ë³€ê²½
            }
    else:
        # ì „ì²´ ì‚¬ìš©ìì˜ ë³´í—˜ í†µê³„
        for insurance_type in INSURANCE_TYPES:
            total_count = db.query(Insurance).filter(Insurance.type == insurance_type).count()
            result["insurance_types"][insurance_type] = {
                "loaded": total_count > 0,
                "docs": total_count
            }
    
    return result


@app.get("/health", summary="ì„œë²„ ìƒíƒœ í™•ì¸")
async def health_check():
    return {"status": "healthy", "message": "ë³´í—˜ ì²­êµ¬ ì‹¬ì‚¬ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ ì¤‘"}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8005)